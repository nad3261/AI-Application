{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nad3261/AI-Application/blob/main/Fetching_data_from_Wikipedia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYGrQ7Yy-plH",
        "outputId": "bf4a8678-7a9b-4850-c738-b9dcd4a0a696"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1MwaFc5-rA_",
        "outputId": "6f642b3b-3916-473f-9f0b-91b347d7ae2e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.8.30)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14347 sha256=6223b266a37a66603856f371fc6323ddafeb4f44dca8740b0e683796097b65b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/96/18/b9201cc3e8b47b02b510460210cfd832ccf10c0c4dd0522962\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix)\n",
        "import wikipediaapi\n",
        "import spacy\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "DUH4ai71-mKd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNkaq_Ts-lq2"
      },
      "outputs": [],
      "source": [
        "user_agent = \"Wikipedia searchign bot\"\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "    language='en',\n",
        "    user_agent=user_agent\n",
        ")\n",
        "\n",
        "def fetch_articles_from_category(category_name, max_depth=1, current_depth=0, max_articles=20):\n",
        "    category = wiki_wiki.page(\"Category:\" + category_name)\n",
        "    articles = []\n",
        "\n",
        "    if not category.exists():\n",
        "        print(f\"Category '{category_name}' does not exist.\")\n",
        "        return articles\n",
        "\n",
        "    for c in category.categorymembers.values():\n",
        "        if len(articles) >= max_articles:\n",
        "            break\n",
        "        if c.ns == wikipediaapi.Namespace.MAIN:\n",
        "            print(f\"Fetching article: {c.title} from {category_name}\")\n",
        "            articles.append((c.title, c.text, category_name))\n",
        "        elif c.ns == wikipediaapi.Namespace.CATEGORY and current_depth < max_depth:\n",
        "            articles.extend(fetch_articles_from_category(c.title[9:], max_depth, current_depth + 1, max_articles - len(articles)))\n",
        "\n",
        "    return articles\n",
        "\n",
        "\n",
        "categories = [\"Mathematics\", \"Biology\", \"Geography\", \"History\", \"Science\"]\n",
        "max_articles_per_category = 100\n",
        "\n",
        "\n",
        "all_articles = []\n",
        "for category in categories:\n",
        "    articles = fetch_articles_from_category(category, max_depth=1, max_articles=max_articles_per_category)\n",
        "    all_articles.extend(articles)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(all_articles, columns=[\"Title\", \"Content\", \"Category\"])\n",
        "\n",
        "df.to_csv(\"category_wikipedia_articles.csv\", index=False)\n",
        "\n",
        "print(f\"{len(all_articles)} articles downloaded and saved to 'category_wikipedia_articles.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoryMapper:\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "        self.category_mapping = {\n",
        "            'Fields of mathematics': 'Mathematics',\n",
        "            'Mathematics-related lists': 'Mathematics',\n",
        "            'Branches of biology': 'Biology',\n",
        "            'Organisms': 'Biology',\n",
        "            'Biologists': 'Biology',\n",
        "            'Biology-related lists': 'Biology',\n",
        "            'Geographers': 'Geography',\n",
        "            'Geography-related lists': 'Geography',\n",
        "            'History by ethnic group': 'History',\n",
        "            'History by period': 'History',\n",
        "            'Fields of history': 'History',\n",
        "            'Historiography': 'History',\n",
        "            'Branches of science': 'Science',\n",
        "            'Scientific disciplines': 'Science',\n",
        "            'Scientists': 'Science',\n",
        "            'Science in society': 'Science'\n",
        "        }\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "    def map_categories(self, df):\n",
        "        #Replace categories in the DataFrame using the mapping.\n",
        "        df['Category'] = df['Category'].replace(self.category_mapping)\n",
        "        return df\n",
        "\n",
        "    def get_unique_categories(self, df):\n",
        "        #Return unique categories after mapping.\n",
        "        return df['Category'].unique()\n",
        "\n",
        "    def remove_stop_words(self, text):\n",
        "        #Remove stop words from the given text.\n",
        "        text = text.lower()\n",
        "        words = text.split()\n",
        "        filtered_words = [word for word in words if word not in self.stop_words]\n",
        "        return ' '.join(filtered_words)\n",
        "\n",
        "    def process_content(self, df):\n",
        "        #Apply stop word removal to the Content column.\n",
        "        df['Content'] = df['Content'].apply(self.remove_stop_words)\n",
        "        return df\n",
        "\n",
        "    def metadata_fix(self, text):\n",
        "        #Clean HTML tags and unwanted characters from the text.\n",
        "        soup = BeautifulSoup(text, \"html.parser\")\n",
        "        cleaned_text = soup.get_text()\n",
        "        cleaned_text = re.sub(r'http\\S+|www\\S+|https\\S+', '', cleaned_text)\n",
        "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', cleaned_text)\n",
        "        return cleaned_text\n",
        "\n",
        "    def fix_metadata(self, df):\n",
        "        #Apply metadata cleaning to the Content column.\n",
        "        df['Content'] = df['Content'].apply(self.metadata_fix)\n",
        "        return df\n",
        "\n",
        "    def lemmatize_words(self, text):\n",
        "        #Lemmatize words in the given text.\n",
        "        words = word_tokenize(text)\n",
        "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
        "        return ' '.join(lemmatized_words)\n",
        "\n",
        "    def lemmatize_content(self, df):\n",
        "        #Apply lemmatization to the Content column\n",
        "        df['Content'] = df['Content'].apply(self.lemmatize_words)\n",
        "        return df\n",
        "\n",
        "    def tokenize_content_spacy(self, df):\n",
        "        #Tokenize content using spaCy.\n",
        "        df['Content'] = df['Content'].apply(lambda x: [token.text for token in self.nlp(x)])\n",
        "        return df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CGwO4rctM0qw"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}