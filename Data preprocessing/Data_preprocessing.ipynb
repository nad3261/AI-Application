{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix)\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsi7vwTDUejD",
        "outputId": "2ae8f2b6-d583-40d2-c23d-7992944946e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-HCYJe-jUbyc"
      },
      "outputs": [],
      "source": [
        "class DataPreprocessing:\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "        self.category_mapping = {\n",
        "            'Fields of mathematics': 'Mathematics',\n",
        "            'Mathematics-related lists': 'Mathematics',\n",
        "            'Branches of biology': 'Biology',\n",
        "            'Organisms': 'Biology',\n",
        "            'Biologists': 'Biology',\n",
        "            'Biology-related lists': 'Biology',\n",
        "            'Geographers': 'Geography',\n",
        "            'Geography-related lists': 'Geography',\n",
        "            'History by ethnic group': 'History',\n",
        "            'History by period': 'History',\n",
        "            'Fields of history': 'History',\n",
        "            'Historiography': 'History',\n",
        "            'Branches of science': 'Science',\n",
        "            'Scientific disciplines': 'Science',\n",
        "            'Scientists': 'Science',\n",
        "            'Science in society': 'Science'\n",
        "        }\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "    def map_categories(self, df):\n",
        "        #Replace categories in the DataFrame using the mapping.\n",
        "        df['Category'] = df['Category'].replace(self.category_mapping)\n",
        "        return df\n",
        "\n",
        "    def get_unique_categories(self, df):\n",
        "        #Return unique categories after mapping.\n",
        "        return df['Category'].unique()\n",
        "\n",
        "    def remove_stop_words(self, text):\n",
        "        #Remove stop words from the given text.\n",
        "        text = text.lower()\n",
        "        words = text.split()\n",
        "        filtered_words = [word for word in words if word not in self.stop_words]\n",
        "        return ' '.join(filtered_words)\n",
        "\n",
        "    def process_content(self, df):\n",
        "        #Apply stop word removal to the Content column.\n",
        "        df['Content'] = df['Content'].apply(self.remove_stop_words)\n",
        "        return df\n",
        "\n",
        "    def metadata_fix(self, text):\n",
        "        #Clean HTML tags and unwanted characters from the text.\n",
        "        soup = BeautifulSoup(text, \"html.parser\")\n",
        "        cleaned_text = soup.get_text()\n",
        "        cleaned_text = re.sub(r'http\\S+|www\\S+|https\\S+', '', cleaned_text)\n",
        "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', cleaned_text)\n",
        "        return cleaned_text\n",
        "\n",
        "    def fix_metadata(self, df):\n",
        "        #Apply metadata cleaning to the Content column.\n",
        "        df['Content'] = df['Content'].apply(self.metadata_fix)\n",
        "        return df\n",
        "\n",
        "    def lemmatize_words(self, text):\n",
        "        #Lemmatize words in the given text.\n",
        "        words = word_tokenize(text)\n",
        "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
        "        return ' '.join(lemmatized_words)\n",
        "\n",
        "    def lemmatize_content(self, df):\n",
        "        #Apply lemmatization to the Content column\n",
        "        df['Content'] = df['Content'].apply(self.lemmatize_words)\n",
        "        return df\n",
        "\n",
        "    def tokenize_content_spacy(self, df):\n",
        "        #Tokenize content using spaCy.\n",
        "        df['Content'] = df['Content'].apply(lambda x: [token.text for token in self.nlp(x)])\n",
        "        return df\n",
        "\n"
      ]
    }
  ]
}